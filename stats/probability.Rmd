---
title: "Probability"
author: "Jarod G.R. Meng"
date: "2/14/2021"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
    df_print: paged
  pdf_document: default
---

# Probability

We need to understand the concept of a *random variable*. We first define the notion of random variable formally before we introduce intuitive examples.

A random variable, which will be denoted as $X$, is defined as a *function* from a sample space of possible outcomes $S$ to the real number system:

$$
X : S \rightarrow \mathbb{R}
$$

The random variable (i.e., the *function*) associates each outcome $\omega$ in the sample space $S$ ($S(\omega \in S)$) with exactly one number $X(\omega) = x$.

$S_X$ represents a set that contains all the $x$'s (i.e., all the possible values of $X$, or simply the *support* of $X$). We can thus write: $x \in S_X$.

For example, a random variable might be the outcome of flipping a fair coin. In this case, the sample space contains two possible outcomes: tail or head. The random variable might map head to number 1 and tail to number 0.

$$
X(head)=1 \\
X(tail)=0 \\
S_X = \{0, 1\}
$$

Every random variable $X$ has an associated probability mass (or distribution) function (i.e., PMF or PDF). PMF is used for *discrete* distributions and PDF is for *continuous* distributions. The PMD/PDF maps every element of $S_X$ to a value between 0 and 1.

$$
p_X: S_X \rightarrow [0, 1]
$$

## Discrete probability distributions

Following the previous simple example of flipping a fair coin. Imagine that we're interested in knowing how many flips return heads in 20 flips. The sample space has 21 possible outcomes, ranging from 0 (i.e., no heads at all) to 20 (i.e., all heads).

$$
S = \{0, 1, 2, ..., 20\}
$$

Since the outcome themselves are real numbers, the mapping is rather straightforward. We can simply map the outcomes to themselves and call it a random variable $X$.

$$
S_X = \{0, 1, 2, ..., 20\}
$$

The probability of each possible $x$ value ($x \in S_X$) depends on the likelihood of each flip returning head (let's assume that all flips are independent from each other and the likelihood stays the same), but the probability must fall into the range $[0,1]$.

$$
p_X = prob(x) \in [0, 1]
$$

This distribution is called a *binomial* distribution and it's a classic example of *discrete* probability distributions. It's discrete because the sample space $S_X$ only contains individual elements and thus can be enumerated in a discrete way.

The probability mass function $p_X$ depends on an important factor, namely, the likelihood of each independent flip returning head. We call those important factors of a probability distribution its **parameters**. For commonly used distributions, their parameters denotations usually follow certain conventions (e.g., a binomial distribution is parameterized using $n$ and $p$, etc). For a generic distribution, its parameter is usually denoted as $\theta$.

### Binomial distribution

The binomial distribution is a discrete probability distribution of the number of successes in a sequence of $n$ *independent* experiments, each asking a yes-no question, and each with its own boolean-valued outcome: success (with probability $p$) or failure (with probability $q=1-p$).

```{r include=FALSE}
library(tidyverse)
theme_set(theme_minimal())
```

A sample binomial distribution's probability mass function "curve" is shown below. Since it's a discrete distribution, there is no continuous density curve, but rather discrete probabilities for possible outcomes (i.e., the vertical bars in the graph).

```{r}
tibble(
  x = seq(0, 20, 1),
  px = dbinom(x, size = 20, prob = 0.5)
) %>%
  ggplot(aes(x = x, y = px)) +
    geom_col(alpha = 0.25, width = 0.5) +
    geom_point() +
    geom_line(linetype = "dashed", alpha = 0.5) +
    labs(
      x = expression(X %~% B(20, 0.5)),  y = "probability density",
      title = "Binomial distribution P.M.F"
    )
```

The support (i.e., all possible values of $x$) $k \in \{0, 1, 2, ..., n\}$. For example, in the binomial distribution $B(20, 0.5)$ above, the support is all integer numbers from 0 to 20.

A binomial distribution is not always symmetrical. Below we show a few binomial distributions with the same $n=20$ size parameter, but different $p$ parameter. Only the distribution $B(20, 0.5)$ is symmetrical. Others are skewed depending on the $p$ values.

```{r}
binom_density <- function(p) {
  tibble(
    x = seq(0, 20, 1),
    p = p,
    px = dbinom(x, size = 20, prob = p)
  )
}
ps <- c(0.1, 0.25, 0.5, 0.75, 0.9)
map_dfr(ps, binom_density) %>%
  mutate(
    p = factor(p, levels = ps)
  ) %>%
  ggplot(aes(x = x, y = px)) +
    geom_point(aes(color = p)) +
    geom_line(aes(color = p), linetype = "dashed", alpha = 0.5) +
    labs(
      x = expression(X %~% B(20, p)),  y = "probability density",
      title = "Binomial distribution P.D.F"
    )
```

The probability mass function of a binomial distribution can be computed deterministically using the $n$ and $p$ parameters.

$$
f(k, n, p) = Pr(k; n, p) = Pr(X=k) = {n \choose k} \cdot p^k \cdot (1-p)^{(n-k)} = {n \choose k} \cdot p^k \cdot q^{(n-k)}
$$
where

$$
{n \choose k} = \frac{n!}{k! \cdot (n-k)!}
$$
is the binomial coefficient, which equals to the number of ways $k$ successes can be distributed among the total $n$ experiments.

```{r}
binom_cume_density <- function(p) {
  tibble(
    x = seq(0, 20, 1),
    p = p,
    px = pbinom(x, size = 20, prob = p)
  )
}
ps <- c(0.1, 0.25, 0.5, 0.75, 0.9)
map_dfr(ps, binom_cume_density) %>%
  mutate(
    p = factor(p, levels = ps)
  ) %>%
  ggplot(aes(x = x, y = px)) +
    geom_point(aes(color = p)) +
    geom_line(aes(color = p), linetype = "dashed", alpha = 0.5) +
    labs(
      x = expression(X %~% B(20, p)),  y = "cumulative probability density",
      title = "Binomial distribution C.D.F"
    )
```

### Poisson distribution

The Poisson distribution is another "famous" discrete probability distribution, named after French mathematician SimÃ©on Denis Poisson. It expresses a given number of events occurring in a *fixed* interval of time or space if **these events occur with a known constant mean rate and independently from each other**. Examples are the number of calls a call center receives in a fixed 30-minute window, the number of buses that arrive at a bus stop within an hour, etc.

The Poisson distribution is parameterized using only one parameter: the constant mean rate of events. We usually denote the parameter $\lambda$. Note that its unit is *events/interval* because it's a rate.

If a discrete variable $X$ is said to have a Poisson distribution with parameter $\lambda > 0$, the probability mass function of $X$ is given by:

$$
f(k, \lambda) = Pr(X=k)=\frac{\lambda^k \cdot e^{-\lambda}}{k!}
$$
Here is an example in which the parameter $\lambda=4$.

```{r}
tibble(
  x = seq(0, 20, 1),
  px = dpois(x, lambda = 4)
) %>%
  ggplot(aes(x = x, y = px)) +
    geom_col(alpha = 0.25, width = 0.5) +
    geom_point() +
    geom_line(linetype = "dashed", alpha = 0.5) +
    labs(
      x = expression(X %~% Pois(4)),  y = "probability density",
      title = "Poisson distribution P.M.F"
    )
```

We can also show multiple Poisson PMF "curves" with different parameter $\lambda$ values in one chart for comparison.

```{r}
poisson_density <- function(lambda) {
  tibble(
    x = seq(0, 20, 1),
    lambda = lambda,
    px = dpois(x, lambda = lambda)
  )
}
lambdas <- c(1, 2, 4, 10)
map_dfr(lambdas, poisson_density) %>%
  mutate(
    lambda = factor(lambda, levels = lambdas)
  ) %>%
  ggplot(aes(x = x, y = px)) +
    geom_point(aes(color = lambda)) +
    geom_line(aes(color = lambda), linetype = "dashed", alpha = 0.5) +
    labs(
      x = expression(X %~% Pois(lambda)),  y = "probability density",
      title = "Poisson distribution P.D.F"
    )
```

The cumulative distribution functions are shown below.

```{r}
poisson_cume_density <- function(lambda) {
  tibble(
    x = seq(0, 20, 1),
    lambda = lambda,
    px = ppois(x, lambda = lambda)
  )
}
lambdas <- c(1, 2, 4, 10)
map_dfr(lambdas, poisson_cume_density) %>%
  mutate(
    lambda = factor(lambda, levels = lambdas)
  ) %>%
  ggplot(aes(x = x, y = px)) +
    geom_point(aes(color = lambda)) +
    geom_line(aes(color = lambda), linetype = "dashed", alpha = 0.5) +
    labs(
      x = expression(X %~% Pois(lambda)),  y = "cumulative probability density",
      title = "Poisson distribution C.D.F"
    )
```

### Relationship between binomial and Poisson distributions

The Poisson distribution can actually be considered to be a special case of binomial distribution in which $n$ is very large and $p$ very small and $\lambda = n \cdot p$ is constant within a unit of interval (recall that Poisson's parameter $\lambda$ is a rate).

The binomial distribution's probability mass function is the following.

$$
f(k, n, p) = {n \choose k} \cdot p^k \cdot (1-p)^{(n-k)}
$$
Since we assume that $\lambda = n \cdot p$, we can write $p = \frac{\lambda}{n}$ and substitute it into the formula above.

$$
\begin{aligned}
f(k, n, \lambda) &= {n \choose k} \cdot (\frac{\lambda}{n})^k \cdot (1-\frac{\lambda}{n})^{(n-k)} \\
&= (\frac{n!}{k! \cdot (n-k)!} \cdot \frac{1}{n^k}) \cdot \lambda^k \cdot (1-\frac{\lambda}{n})^n \cdot (1-\frac{\lambda}{n})^{-k}
\end{aligned}
$$

What we're interested in is the case when $n$ approaches infinity.

$$
f(k, \lambda) = \lim_{n \to +\infty} (\frac{n!}{(n-k)!} \cdot \frac{1}{n^k}) \cdot \frac{\lambda^k}{k!} \cdot (1-\frac{\lambda}{n})^n \cdot (1-\frac{\lambda}{n})^{-k}
$$
The first term $\frac{n!}{(n-k)!} \cdot \frac{1}{n^k}$ can be expanded to make it easier to compute the limit.

$$
\begin{aligned}
\lim_{n \to +\infty} \frac{n!}{(n-k)!} \cdot \frac{1}{n^k} &= \lim_{n \to +\infty} \frac{n \cdot (n-1) \cdot ... \cdot (n-k+1)}{n^k} \\
&= \lim_{n \to +\infty} \frac{n}{n} \cdot \frac{n-1}{n} \cdot ... \cdot \frac{n-k+1}{n} \\
&= 1
\end{aligned}
$$

The third term $(1-\frac{\lambda}{n})^n$ requires the Euler's number $e$.

Recall that

$$
e = \lim_{x \to \pm\infty} (1+\frac{1}{x})^x
$$

Let $x = -\frac{n}{\lambda}$, then

$$
\begin{aligned}
\lim_{n \to +\infty} (1-\frac{\lambda}{n})^n &= \lim_{x \to -\infty} (1+\frac{1}{x})^{x \cdot -\lambda} \\
&= [\lim_{x \to -\infty} (1+\frac{1}{x})^x]^{-\lambda} \\
&= e^{-\lambda}
\end{aligned}
$$

The last term $(1-\frac{\lambda}{n})^{-k}$ approaches 1 when $n$ approaches infinity.

Therefore,

$$
\begin{aligned}
f(k, \lambda) &= \lim_{n \to +\infty} (\frac{n!}{(n-k)!} \cdot \frac{1}{n^k}) \cdot \frac{\lambda^k}{k!} \cdot (1-\frac{\lambda}{n})^n \cdot (1-\frac{\lambda}{n})^{-k} \\
&= 1 \cdot \frac{\lambda^k}{k!} \cdot e^{-\lambda} \cdot 1 \\
&= \frac{\lambda^k \cdot e^{-\lambda}}{k!}
\end{aligned}
$$

and that's exactly what the Poisson distribution's probability mass function is.

## Continuous probability distributions

### Normal distribution {#normal_dist}

#### Definitions

A **normal** (or **Gaussian**) distribution is a *continuous* probability distribution.

The general form of its **probability density function** is:

$$
f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
$$

The parameter $\mu$ is the expected value or mean and $\sigma$ is the standard deviation. The variance of the distribution is $\sigma^2$.

The normal distribution is often referred to as $N(\mu,\sigma^2)$.

The simplest case of a normal distribution is known as the *standard normal distribution*. This is a special case when $\mu=0$ and $\sigma=1$. It can be described by the following probability density function.

$$
\varphi(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}
$$

The shape of the probability density function is a **symmetric (around the mean) bell curve**. Its symmetry comes from the $x^2$ term. The negative sign in $-\frac{1}{2}x^2$ makes the curve greater in the middle and smaller on the sides.

```{r}
ggplot() +
  xlim(-3, 3) +
  geom_function(fun = dnorm) +
  labs(
    x = expression(X %~% N(0, 1)), y = "probability density",
    title = "Normal distribution P.D.F"
  )
```

The **cumulative distribution function** of the standard normal distribution, usually denoted with the capital Greek letter $\Phi$, is the integral.

$$
\Phi(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}e^{-t^2/2}dt
$$

```{r}
ggplot() +
  xlim(-3, 3) +
  geom_function(fun = pnorm) +
  labs(
    x = expression(X %~% N(0, 1)), y = "probability density",
    title = "Normal distribution C.D.F"
  )
```

The quantile function is the inverse of the cumulative distribution function. The quantile function of a standard normal distribution is called the **probit** function. It's denoted as $\Phi^{-1}(x)$.

```{r}
ggplot() +
  xlim(0, 1) +
  geom_function(fun = qnorm) +
  labs(
    x = "density", y = expression(X %~% N(0, 1)),
    title = "Normal distribution quantile function"
  )
```

The quantile $\Phi^{-1}(p)$ of the standard normal distribution is commonly denoted as $z_{p}$. These values are used in hypothesis testing, construction of confidence intervals, and Q-Q plots. In particular, $Z_{0.975}=1.96$ and $Z_{0.025}=-1.96$. They are often used to construct confidence intervals of 95% significant level.

#### Linear combination of two or more random variables

If $X_1$ and $X_2$ are two independent standard normal random variables with mean 0 and variance 1, then

1. Their sum and difference is distributed normally with mean 0 and variance 2

$$
X_1 \pm X_2 \sim N(0,2)
$$

1. If $X_1$, $X_2$, ..., $X_n$ are independent standard normal random variables, then the sum of their squares has the **chi-squared distribution** with $n$ degrees of freedom.

$$
X_1^2+...+X_n^2 \sim \chi_n^2
$$
