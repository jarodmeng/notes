---
title: "Basic statistics"
output:
  html_document:
    toc: true
    toc_depth: 5
    toc_float: true
    df_print: paged
  pdf_document: default
---

# Probability

## Probability distributions

### Normal distribution {#normal_dist}

#### Definitions

A **normal** (or **Gaussian**) distribution is a *continuous* probability distribution.

The general form of its **probability density function** is:

$$
f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}
$$

The parameter $\mu$ is the expected value or mean and $\sigma$ is the standard deviation. The variance of the distribution is $\sigma^2$.

The normal distribution is often referred to as $N(\mu,\sigma^2)$.

The simplest case of a normal distribution is known as the *standard normal distribution*. This is a special case when $\mu=0$ and $\sigma=1$. It can be described by the following probability density function.

$$
\varphi(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}
$$

The shape of the probability density function is a symmetric (around the mean) bell curve. Its symmertry comes from the $x^2$ term. The negative sign in $-\frac{1}{2}x^2$ makes the curve greater in the middle and smallers on the sides.

```{r}
x <- seq(from = -6, to = 6, by = 0.001)
plot(x, dnorm(x), cex = 0.25)
```

The **cumulative distribution function** of the standard normal distribution, usually denoted with the capital Greek letter $\Phi$, is the integral.

$$
\Phi(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}e^{-t^2/2}dt
$$

```{r}
x <- seq(from = -6, to = 6, by = 0.001)
plot(x, pnorm(x), cex = 0.25)
```

The quantile function is the inverse of the cumulatie distribution function. The quantile function of a standard normal distribution is called the **probit** function. It's denoted as $\Phi^{-1}(x)$.

```{r}
x <- seq(from = 0, to = 1, by = 0.00001)
plot(x, qnorm(x), cex = 0.25)
```

The quantile $\Phi^{-1}(p)$ of the standard normal distribution is commonly denoted as $z_{p}$. These values are used in hypothesis testing, construction of confidence intervals, and Q-Q plots. In particular, $Z_{0.975}=1.96$ and $Z_{0.025}=-1.96$. They are often used to construct confidence intervals of 95% significant level.

#### Linear combination of two or more random variables

If $X_1$ and $X_2$ are two independent standard normal random variables with mean 0 and variance 1, then

1. Their sum and difference is distributed normally with mean 0 and variance 2

$$
X_1 \pm X_2 \sim N(0,2)
$$

1. If $X_1$, $X_2$, ..., $X_n$ are independent standard normal random variables, then the sum of their squares has the **chi-squared distribution** with $n$ degrees of freedom.

$$
X_1^2+...+X_n^2 \sim \chi_n^2
$$

# Statistics

Assuming there's a population. Let's first assume that the underlying distribution is a normal distribution. We can simulate 10,000 such values easily.

```{r}
set.seed(42)
N <- 10000
A <- rnorm(n = N, mean = 10, sd = 2)
```

## Population mean

Mean is the **expected value of an element** in the population. Since it's computed over a population, it's a **parameter** of the population.

### Interpretations

There are two interpretations of the definiton.

#### A single population interpretation

When we have access to all elements in a population, we can compute the expected value of an element using the whole data.

$$
\mu = E(A_i) = \sum_{i=1}^{N}(A_ip_i)
$$

Here $A_i$ represents one element of the population.

Since each element in the population has an equal probability (or weight), each of them has a $\frac{1}{N}$ probability, namely, $p_i = \frac{1}{N}$.

Therefore,

$$
\mu = \sum_{i=1}^{N}(\frac{A_i}{N}) = \frac{\sum_{i=1}^{N}A_i}{N}
$$

```{r}
(MU <- sum(A * (1/N)))
```

#### A random variable interpretation

Another interpretation is that the population mean is the expected value of an element randomly drawn from the population.

Here we don't assume that we have access to the entire population, but we have a way of observing a random element of the population. This interpretation is closer to reality in which we never really see an entire population.

Under this interpretation, we actually can't compute the exact value of $\mu$ as we don't have coverage of all elements in the population (we actually don't even know the size of the population). Instead, the population mean $\mu$ is simply defined as the expected value of a randonly drawn element of the population.

$$
\mu = E(X \in A)
$$

Here $X$ represents a random variable that corresponds to an element of the population. We can repeat the observation process many times and each time it would give us one realization of the random variable $X$ as $X_i$. $\mu$ is the expected value of all those $X_i$ values.

#### Squared-differences-minimizing property

Besides being the expected value of an element in the population, the population mean actually has an interesting property whereby it minimizes the sum of squared differences.

Assume that there's a function $SSD$ that takes a value and computes the sum of squared differences between each element of the population and that supplied value.

$$
SSD(x) = \sum_{i=1}^{N}(A_i - x)^2
$$

The populatio mean's squared-differences-minimizing property means

$$
SSD(\mu) <= SSD(\text{any number})
$$

To find the value that minimizes the sum of squared differences, we can set the derivatie of $SSD(x)$ to 0 and solve for $x$.

$$
\begin{aligned}
&\frac{d}{dx}(\sum_{i=1}^{N}(A_{i}-x)^2) = 0 \\
&\frac{d}{dx}(\sum_{i=1}^{N}(A_{i}^2 - 2x \cdot A_{i} + x^2)) = 0 \\
&\frac{d}{dx}(\sum_{i=1}^{N}A_{i}^2-2x \cdot \sum_{i=1}^{N}A_{i}+Nx^2) = 0 \\
&-2\sum_{i=1}^{N}A_{i}+2Nx = 0 \\
&x = \frac{\sum_{i=1}^{N}A_{i}}{N}
\end{aligned}
$$
That is, **the population mean minimizes the sum of squared differences**.

We can use simulation to verify this property of the mean.

```{r}
# Generate a lot of possible $a$ values
X <- seq(from = 8, to = 12, by = 0.0001)
sum_squared_diff <- sapply(X, function(x) sum((A-x)^2))
plot(X, sum_squared_diff, cex = 0.25)
```

We can see that the $x$ value that minimizes the sum of squared differences is around 10, but it's hard to pinpoint its value. We can find its precise value using the simulation result.

```{r}
(x <- X[which(sum_squared_diff == min(sum_squared_diff))])
```

We can verify that the $x$ value we found is indeed equal to the mean.

```{r}
all.equal(x, MU, tolerance = 0.001)
```

```{r include=FALSE}
rm(X)
rm(sum_squared_diff)
```

##### Median's minimizing property

While the sum of squared differences is a common way of measuring the total distances between a set of numbers and a value, a more straightforward choice is the absolute value of difference, i.e.

$$
|A_i - x|
$$
We can also sum the absolute differences over all elements of the population.

$$
\sum_{i=1}^{N}(|{A_i-x}|)
$$

In the case of a discrete population, the value that minimizes the sum of absolute
differences is actually the **median** of the population, i.e., the *middle* element. [This question exchange](https://math.stackexchange.com/questions/113270/the-median-minimizes-the-sum-of-absolute-deviations-the-l-1-norm) provides some proof for this result.

We can verify that it's indeed the median that minimizes the sum.

```{r}
# Generate a lot of possible alpha values
X <- seq(from = 8, to = 12, by = 0.0001)
sum_abs_diff <- sapply(X, function(x) sum(abs(A-x)))
(x <- mean(X[which(sum_abs_diff == min(sum_abs_diff))]))
```

```{r}
all.equal(median(A), x, tolerance = 0.001)
```

```{r include=FALSE}
rm(X)
rm(sum_abs_diff)
```

## Population variance

Variance is the **expectation of the squared deviation** of a random variable to its mean. We can compute the variance for the population. It's also a **parameter** of the population.

$$
\sigma^2 = E[(A_i - \mu)^2]
$$

$$
\begin{aligned}
\sigma^2 &= E[(A_i-E[A_i])^2] \\
&= E[A_i^2 - 2A_iE[A_i] + (E[A_i])^2]] \\
&= E[A_i^2] - 2E[A_i]E[A_i] + (E[A_i])^2 \\
&= E[A_i^2] - (E[A_i])^2 \\
&= E[A_i^2] - \mu^2
\end{aligned}
$$

We can compute the population variance according to its definition.

```{r}
(VAR <- sum((A - MU)^2 * (1/N)))
```

As we noted in the section above, the mean $\mu$, defined as the expected value of $X$, minimizes the sum of squared differences. Since the population size $N$ is a fixed number, this means that the variance is the smallest expected squared difference. Using any other number to replace $\mu$ in the formula would result in a greater variance.

Alternatively, we can compute the population variance according to the final transformation above.

```{r}
(VAR.2 <- sum(A^2 * (1/N)) - MU^2)
```

We can see that `VAR` and `VAR.2` are equal.

R provides a `var` function whose name might suggest that it computes the population variance. We can call it on our population $X$ to see what it outputs.

```{r}
(VAR.R <- var(A))
```

We can see that the calculated variance is greater than `VAR` and `VAR.2`. This is because by default R treats the variance as sample variance rather than population variance and it's using a slightly different formula. The reason why sample variance is calculated using a different formula is that the sample variance is a biased estimator of the population variance and needs to corrected by multiplying a scale factor $\frac{N}{N-1}$. The section below covers more generally on the topic of estimators.

```{r}
all.equal(VAR * N/(N-1), VAR.R)
```

## Estimators

In real life, we almost never get to see the whole population, but often a sample drawn from the population.

A **parameter** refers to any characteristic of a **population**. When it's not possible or practical to directly measure the value of a population parameter, statistical methods are used to infer the likely value on the basis of a **statistic** computed from a **sample** taken from the population.

When a statistic is used to *estimate* a population parameter, it's called an **estimator**. We can think of the estimator as a function that maps the sample space to a set of sample estimates. For example, $f$ is a function that works on sample data $X$, and can be expressed as $f(X)$. $f$ here is the estimator and its result (which can be a point or an interval) is the estimate. We can apply the same function on many sample data sets to get many sample estimates.

### Conventions

To discuss the properties of an estimator, we need to first establish some conventions.

We usually name the fixed parameter (of a population) that needs to be estimated as $\theta$. The estimator of $\theta$ is usually denoted by the symbol $\hat{\theta}$. As discussed above, if $X$ is used to denote a random variable corresponding to the sample data, the estimator can be symbolized as a function of that random variable, $\hat{\theta}(X)$. The estimate result for a particular sample data $x$ is then $\hat{\theta}(x)$.

### Error

For a given sample $x$, the **error** of the estimator $\theta$ is defined as

$$
e(x) = \hat{\theta}(x) - \theta
$$

It's the difference between the estimate obtained from applying the estimator (a function) on the sample data $x$ and the true population parameter.

### Mean squared error (MSE)

The MSE of an estimator is defined as the **exepcted value of the squared error**.

$$
MSE(\hat{\theta}) = E[(\hat{\theta}(X)-\theta)^2]
$$

Here $X$ is a random variable corresponding to the sample space. It can be realized as observed sample data, $x_1, x_2, ..., x_n$, each represents a sample. When the estimator is applied onto all those samples, it produces a set of estimates each of which has an error against the true parameter. MSE is the expected value (or weighted average) of the square of those errors.

Suppose the parameter is the bull's-eye of a target, the estimator is the process of shooting arrows at the target, and the individual arrows are estimates (samples). Then MSE is the *average* distance of the arrows from the bull's eye.

### Sampling deviation

For a given sample $x$, the sampling deviation of the estimator $\hat{\theta}$ is defined as

$$
d(x) = \hat{\theta}(x) - E[\hat{\theta}(X)] = \hat{\theta}(x) - E(\hat{\theta})
$$

It's the difference between an estimate and the expected value of all estimates from the estimator.

### Variance

The variance of $\hat{\theta}$ is simply the **expected value of the squared sampling deviations**.

$$
var(\hat{\theta}) = E[(\hat{\theta}-E(\hat{\theta}))^2]
$$

Here $\hat{\theta}$ is treated as a random variable that's the result of applying the estimator $\hat{\theta}$ on the sample space random variable.

If the parameter is the bull's-eye of a target, and the arrows are estimates, then the variance measures how dispersed those arrows are from each other. *It has nothing to do with the true parameter $\theta$*. That is, if all arrows hit the same point but the point is nowhere near the bull's eye, the variance is still 0.

The square root of the variance is called the **standard error* of $\hat{\theta}$. Note that the term *error* here has nothing to do with the true parameter $\theta$.

### Bias

The bias of $\hat{\theta}$ is defined as

$$
B(\hat{\theta}) = E(\hat{\theta}) - \theta
$$

It's the difference between the expected value of all estimates from the estimator and the true parameter. Note that the bias is not a fixed value, but a *function* of the true parameter value $\theta$.

Since parameter $\theta$ is a fixed value, it's its own expected value. That is,

$$
E(\theta) = \theta
$$

We can express the bias of an esimator as the **expected value of the error**.

$$
B(\hat{\theta}) = E(\hat{\theta}) - E(\theta) = E(\hat{\theta}-\theta)
$$

If the parameter is the bull's-eye of a target, and the arrows are estimates, then the bias is the difference between the average arrow and the bull's eye.

### Relationship among the quantities

$$
MSE(\hat{\theta}) = var(\hat{\theta}) + (B(\hat{\theta}))^2
$$

#### Proof

$$
\begin{aligned}
var(\hat{\theta}) + (B(\hat{\theta}))^2 &= E[(\hat{\theta}-E(\hat{\theta}))^2] + [E(\hat{\theta}) - \theta]^2 \\
&= E[\hat{\theta}^2 - 2\hat{\theta}E(\hat{\theta}) + [E(\hat{\theta})]^2] + [E(\hat{\theta})]^2 - 2E(\hat{\theta})\theta + \theta^2 \\
&= E(\hat{\theta}^2) - 2[E(\hat{\theta})]^2] + [E(\hat{\theta})]^2 + [E(\hat{\theta})]^2 - 2E(\hat{\theta})\theta + \theta^2 \\
&= E(\hat{\theta}^2) - 2E(\hat{\theta})\theta + \theta^2 \\
&= E(\hat{\theta}^2 - 2\hat{\theta}\theta + \theta^2) \\
&= E[(\hat{\theta}-\theta)^2] \\
&= MSE(\hat{\theta})
\end{aligned}
$$

## Sample mean and variance

As noted above, we rarely work with the whole population, but often samples drawn from the population. As a result, we often use an estimator (i.e., a function) with the sample data to *infer* population parameters. By far the most common parameters to be estimated are the mean $\mu$ and the variance $\sigma^2$.

Following the circumflex notation introduced in the section above, we can call the mean and variance estimates $\hat{\mu}$ and $\hat{\sigma^2}$ respectively.

### Sample mean

$$
\hat{\mu} = E(X_i \in A) = \frac{\sum_{i=1}^{n}(X_i)}{n}
$$

Here $X$ represents an $n$-element sample drawn from $A$.

#### Bias

To see if the sample mean is an unbiased estimator of the population, we need to work out the expected value of $\hat{\mu}$.

$$
E(\hat{\mu}) = \frac{1}{n}E(\sum_{i=1}^{n}X_i)
$$

##### Proof 1

Using the linear operator property of expectation, we get

$$
\begin{aligned}
E(\hat{\mu}) &= \frac{1}{n}E(\sum_{i=1}^{n}X_i) \\
&= \frac{1}{n}\sum_{i=1}^{n}E(X_i) \\
&= \frac{1}{n}\sum_{i=1}^{n}\mu \\
&= \frac{1}{n} \cdot n \cdot \mu \\
&= \mu
\end{aligned}
$$

The important understanding here is that $E(X_i) = \mu$ because $X_i$ is randomly drawn from the population. We can think of the expectation operator $E$ here as taking an element of the population out at a time but for many many times.

##### Proof 2

There's also an alternative way of proving that sample mean estimator is an unbiased esimator of the population mean. But this proof has its limitations.

Under the assumption that $X_i$ is randomly drawn (**without replacement**) from the population $A$, we can transform the sum of sample elements to the sum of product of population elements and a probability term.

$$
\sum_{i=1}^{n}X_i = \sum_{i=1}^{N}A_iZ_i
$$

In the formula above, $Z_i$ is a binary random variable of 0 or 1. It indicates if a particular population element $A_i$ is excluded or included in the sample $X$. We can think of $Z$ as a binary switch that randomly decide which population units to be drawn into the sample. This line of reasoning breaks down when the sampling is done with replacement since it's equivalent to $Z_i$ being a binary switch.

To compute the overall expected value, we need to figure out the probablity of $Z_i$ equal to 1, namely $Pr(Z_i = 1)$. Note that the probability of $Z_i$ equal to 0 is simply the complement, namely, $Pr(Z_i = 0) = 1 - Pr(Z_i = 1)$, because $Z_i$ can only take a value of 0 or 1.

The probability $Pr(Z_i = 1)$ is a fraction. The numerator is the total number of ways element $A_i$ is included in the $n$-element sample. The denominator is the total number of ways an $n$-element sample can be drawn from the $N$-element population.

In total, there are $\binom{N}{n}$ ways of drawing a sample (without replacement) of size $n$ from a population of size $N$.

The trick to compute the numerator is to look at the remaining $n-1$ elements in the sample of which $A_i$ is already included. There are in total $\binom{N-1}{n-1}$ ways to draw $n-1$ elements from $N-1$ elements in the population.

Therefore,

$$
Pr(Z_i = 1) = \frac{\binom{N-1}{n-1}}{\binom{N}{n}} = \frac{\frac{(N-1)!}{(n-1)!(N-n)!}}{\frac{N!}{n!(N-n)!}} = \frac{\frac{(N-1)!}{(n-1)!}}{\frac{N!}{n!}} = \frac{\frac{(N-1)!}{(n-1)!}}{\frac{N(N-1)!}{n(n-1)!}} = \frac{n}{N}
$$
This makes intuitive sense. When $n=1$, $Pr(Z_i = 1) = \frac{1}{N}$ because the probability of an elment $A_i$ is drawn from the population is the same as the probability of any other element in the population (this is the definition of a random sample), so it's $\frac{1}{N}$. On the other hand, when $n=N$, $Pr(Z_i = 1) = 1$ because such a sample is simply the entire population and because it's sampling without replacement, every element $A_i$ is bound to be included.

Now we can return to derive the expected value of $\hat{\mu}$.

$$
\begin{aligned}
E(\hat{\mu}) &= \frac{1}{n}E(\sum_{i=1}^{n}X_i) \\
&= \frac{1}{n}E(\sum_{i=1}^{N}A_iZ_i) \\
&= \frac{1}{n}\sum_{i=1}^{N}(A_i \cdot 1 \cdot Pr(Z_i = 1) + A_i \cdot 0 \cdot Pr(Z_i = 0)) \\
&= \frac{1}{n}\sum_{i=1}^{N}A_iPr(Z_i = 1) \\
&= \frac{1}{n}\sum_{i=1}^{N}A_i\frac{n}{N} \\
&= \sum_{i=1}^{N}\frac{A_i}{N} \\
&= \mu
\end{aligned}
$$

So the sample mean is an **unbiased estimator** of the population mean.

##### Variance of the sample mean estimator

So far we've established that the expected value (or mean) of the sample mean estimates is the population mean. How about the variance of the sample mean estimates? Recall that here we treat sample means from many $n$-element sample as a random varianble by itself. Every time we draw a different $n$-element sample, we get a differnet sample mean estimates and the collection of many sample mean estimates form the distribution of the random variable.

$$
\begin{aligned}
Var(\hat{\mu}) &= Var(\frac{\sum_{i=1}^{n}X_i}{n}) \\
&=\frac{1}{n^2}\sum_{i=1}^{n}Var(X_i) \\
&=\frac{1}{n^2} \cdot n \cdot \sigma^2 \\
&=\frac{1}{n}\sigma^2
\end{aligned}
$$

That is, the variance of the sample mean estimator is related to the population variance. This makes intuitive sense. If the population is very dispersed, sample means produced from many random samples from the population should tend to be dispersed too.

##### Simulation

We can verify this property by taking many samples (without replacement) from the population.

```{r}
# Sample size
n <- 10
set.seed(42)
# A list of 10-element random samples
X <- replicate(5000, sample(A, n, replace = TRUE), simplify = FALSE)
```

We can compute the means of cumulative samples (which is equivalent to taking average of the means of the samples) to see whether it approaches the population mean.

```{r}
X_mean <- vector("double", length(X))
X_avg_mean <- vector("double", length(X))
X_mean_var <- vector("double", length(X))
for (i in seq_along(X)) {
  X_mean[i] <- mean(X[[i]])
  X_avg_mean[i] <- mean(X_mean[1:i])
  X_mean_var[i] <- var(X_mean[1:i])
}
{
  plot(X_avg_mean, cex = 0.25)
  abline(h = MU, col = "red")
}
```

After 1,000 samples, the cumulative mean indeed approaches the population mean and fluctuates very slightly around it afterwards.

We can also show the distribution of individual sample means.

```{r}
{
  plot(density(X_mean))
  abline(v = MU, col = "red")
}
```

The variance of the sample mean estimates is indeed very close to $\frac{1}{n}$ of the population variance.

```{r}
{
  plot(X_mean_var, cex = 0.25)
  abline(h = VAR/n, col = "red")
}
```

### Sample variance

We can now turn our attention to the sample variance $\hat{\sigma^2}$.

#### Formula and bias

It's natural that we try to mimic the population variance formula here to calculate the sample variance. Since the population mean $\mu$ is unknown to us, we replace it with the sample mean $\hat{\mu}$.

$$
\begin{aligned}
\hat{\sigma^2} &= E[(X_i-\hat{\mu})^2] \\
&= \frac{\sum_{i=1}^{n}(X_i-\hat{\mu})^2}{n}
\end{aligned}
$$

Like the sample mean formula, here $X$ represents one $n$-element sample randomly drawn from the population.

Now we want to figure out if the sample variance estimator is biased or not. Namely, we want to verify if the following equality holds.

$$
E(\hat{\sigma^2}) = \sigma^2
$$

We can start by expanding the formula.

$$
\begin{aligned}
E[\hat{\sigma^2}] &= E[\frac{1}{n}\sum_{i=1}^{n}(X_i-\hat{\mu})^2] &(1)\\
&= E[\frac{1}{n}\sum_{i=1}^{n}[(X_i-\mu)-(\hat{\mu}-\mu)]^2] &(2)\\
&= E[\frac{1}{n}\sum_{i=1}^{n}[(X_i-\mu)^2 - 2(X_i-\mu)(\hat{\mu}-\mu) + (\hat{\mu}-\mu)^2]] &(3)\\
&= E[\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu)^2] - E[\frac{2}{n}\sum_{i=1}^{n}(X_i-\mu)(\hat{\mu}-\mu)] + E[\frac{1}{n}\sum_{i=1}^{n}(\hat{\mu}-\mu)^2] &(4)
\end{aligned}
$$

The reason why we subtract $\mu$ from both $X_i$ and $\hat{\mu}$ in equation (1) is that we're trying to establish some relationship between $E[\hat{\sigma^2}]$ and $\sigma^2$. We know that population variance is computed using the population mean $\mu$.

$$
\begin{aligned}
E[\frac{1}{n}\sum_{i=1}^{n}(X_i-\mu)^2] &= \frac{1}{n} \cdot n \cdot E[(X_i-\mu)^2] \\
&= E[(X_i-\mu)^2] \\
&= \sigma^2
\end{aligned}
$$

Another interesting term in equaltion (4) above is the last term $E[\sum_{i=1}^{n}(\hat{\mu}-\mu)^2]$. This is basically the variance of the sample mean estimator, namely $Var(\hat{\mu})$.

$$
E[\frac{1}{n}\sum_{i=1}^{n}(\hat{\mu}-\mu)^2] = Var(\hat{\mu}) = \frac{1}{n}\sigma^2
$$
Now that we've linked the first and the third term in equation (4) to the population variance, the only term left is the second term, $E[\frac{2}{n}\sum_{i=1}^{n}(X_i-\mu)(\hat{\mu}-\mu)]$.

$$
\begin{aligned}
E[\frac{2}{n}\sum_{i=1}^{n}(X_i-\mu)(\hat{\mu}-\mu)] &= E[\frac{2}{n}(\hat{\mu}-\mu)\sum_{i=1}^{n}(X_i-\mu)] &(1) \\
&=E[\frac{2}{n}(\hat{\mu}-\mu)(\sum_{i=1}^{n}X_i-\sum_{i=1}^{n}\mu)] &(2) \\
&=E[\frac{2}{n}(\hat{\mu}-\mu)(n \cdot \hat{\mu} - n \cdot \mu)] &(3) \\
&=2E[(\hat{\mu}-\mu)^2] &(4) \\
&=2Var(\hat{\mu}) &(5) \\
&=\frac{2}{n}\sigma^2 &(6)
\end{aligned}
$$

Now we can combine the 3 terms together.

$$
\begin{aligned}
E[\hat{\sigma^2}] &= \sigma^2 - \frac{2}{n}\sigma^2 + \frac{1}{n}\sigma^2 \\
&= \sigma^2 - \frac{1}{n}\sigma^2 \\
&= (1-\frac{1}{n})\sigma^2
\end{aligned}
$$

We can also express the equation in another form to give a bit more intuition.

$$
E[\hat{\sigma^2}] - \sigma^2 = -Var(\hat{\mu})
$$

That is, the sample variance estimator has a negative bias of $-Var(\hat{\mu})$. We can correct this bias using a scalar term $\frac{n}{n-1}$.

$$
E[\frac{n}{n-1} \cdot \hat{\sigma^2}] = \frac{n}{n-1} \cdot \frac{n-1}{n} \cdot \sigma^2 = \sigma^2
$$

Since

$$
\hat{\sigma^2} = \frac{\sum_{i=1}^{n}(X_i-\hat{\mu})^2}{n}
$$

We have

$$
\begin{aligned}
\frac{n}{n-1} \cdot \hat{\sigma^2} &= \frac{n}{n-1} \cdot \frac{\sum_{i=1}^{n}(X_i-\hat{\mu})^2}{n} \\
&=\frac{\sum_{i=1}^{n}(X_i-\hat{\mu})^2}{n-1}
\end{aligned}
$$

We usually denote the formula above as $\hat{s^2}$ to differentiate it from $\sigma^2$. We've shown above that $\hat{s^2}$ is an unbiased estimator of the population variance.

#### Simulation

```{r}
X_var1 <- vector("double", length(X))
X_avg_var1 <- vector("double", length(X))
X_var2 <- vector("double", length(X))
X_avg_var2 <- vector("double", length(X))
for (i in seq_along(X)) {
  X_var1[i] <- sum((X[[i]] - mean(X[[i]]))^2)/length(X[[i]])
  X_var2[i] <- var(X[[i]])
  X_avg_var1[i] <- mean(X_var1[1:i])
  X_avg_var2[i] <- mean(X_var2[1:i])
}
{
  plot(X_avg_var1, cex = 0.25)
  points(X_avg_var2, cex = 0.25, col = "blue")
  abline(h = VAR, col = "red")
}
```

The difference between the black line (i.e., $\sigma^2$) and the blue line (i.e., $s^2$) is the bias of the naive sample variance estimator. To be more pricise, it's the bias manifested in this particular simulation case.

#### Variance of sample variance distribution

We can also use the simulation data to show the sample variances' distribution.

```{r}
{
  plot(density(X_var2))
  abline(v = VAR, col = "red")
}
```

## Resampling

So far when we talk about the expected value of a sample statistic (e.g., sample mean), we imagine that we take many $n$-element random samples from the same population (as we did in simulations above) and form a sample statistic distribution (effectively treating the sample statistic as a random variable).

What if we only have one sample, can we derive some sensible estimator of a population parameter using this one sample?

[Resampling](https://en.wikipedia.org/wiki/Resampling_(statistics)) is a set of techniques that can help us here.

### Jackknife resampling

Jackknife is often used in statistical inference to estimate the *bias* and *standard error* of a statistic. The core idea of jackknife is to systematically leave out $k$ (usually $k=1$) observations from the sample to construct $n$ number of $(n-1)$-element subsamples. Each subsample is then used to compute the statistic and the collection of the $n$ statistics are used to compute the overall statistic estimate and the standard error of that estimate (and we can construct the confidence interval).

#### Jackknife sample mean estimator

Let's take sample mean as an example.

Recall when we define the sample mean estimator, we use the following formula.

$$
\hat{\mu} = E(X_i \in A) = \frac{\sum_{i=1}^{n}(X_i)}{n}
$$

Here $X$ is an $n$-element random sample from the population $A$. We show in theory and simulation that if we repeat the process of drawing $n$-element samples from $A$ many (let's say $m$) times, we end up with many (in this case $m$) samples (each of them has $n$ elements). Each sample can be used to compute a sample mean and those $m$ sample means form a sample mean distribution. The expected value of the distribution mean is an unbiased estimator of the population mean.

Let's say that we don't have the luxury of drawing $m$ $n$-element random samples from $A$. We only have one sample $X$. We can somewhat mimic the process above by creating many subsamples from this one sample. The way to do it is to leave one element out each time. That is, the $i$-th subsample from the one sample $X$ is defined as the set

$$
\textbf{X}_{(i)} = \{ X_1, X_2, ..., X_{i-1}, X_{i+1}, ..., X_{n-1}, X_n \}
$$

Note how the $X_i$ element is left out in this subsample above. So the subsample has $n-1$ elements.

For each subsample, we can compute the subsample mean.

$$
\bar{x}_{(i)} = \frac{1}{n-1}\sum_{j=1,j \neq i}^{n}X_j
$$

Repeating this process for $i=1...n$, we will end up with $n$ $\bar{x}_{(i)}$ subsample means. We can then define an overall sample mean jackknife estimator as

$$
\hat{\mu}_{jack} = \frac{1}{n}\sum_{i=1}^{n}\bar{x}_{(i)}
$$

It's straightforward to show that $\hat{\mu}_{jack} = \hat{\mu} = \frac{1}{n}\sum_{i=1}^{n}(X_i)$.

$$
\begin{aligned}
\hat{\mu}_{jack} &= \frac{1}{n} \sum_{i=1}^{n} \bar{x}_{(i)} \\
&= \frac{1}{n} \sum_{i=1}^{n} (\frac{1}{n-1} \sum_{j=1,j \neq i}^{n} X_j) \\
&= \frac{1}{n} \cdot \frac{1}{n-1} \cdot (n-1) \sum_{i=1}^{n} X_i \\
&= \frac{1}{n} \sum_{i=1}^{n} X_i \\
&= \hat{\mu}
\end{aligned}
$$

Since $\hat{\mu}$ is an unbiased estimator of the population mean, the Jackknife sample mean estimator is also an unbiased estimator of the populatio mean. In fact, $\hat{\mu}_{jackknife}$ is exactly the same as $\hat{\mu}$. It's just a different way of computing the same estimate.

#### Jackknife estimate of the sample mean variance

We know that the Jackknife sample mean estimate is an unbiased one. It might seem that Jackknife is a convoluted way of computing the sample mean estimate that we can get from directly computing the sample mean (i.e., rather than taking the mean of the $n$ subsample means, we can simply take the mean of the entire sample and they are equivalent). The benefit of the Jackknife technique comes in when we use the $n$ subsamples to estimate the variance of the sample mean estimate.

We know that the variance of a sample mean estimate is $\frac{1}{n}$ of the population variance.

$$
Var(\hat{\mu}) = \frac{1}{n} \sigma^2
$$

We can't observe the population variable directly. We know that $s^2$ is an unbiased estimator of the population variance $\sigma^2$, so we use the sample $X$ variance.

$$
s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \hat{\mu})^2
$$

Hence,

$$
\begin{aligned}
\hat{Var}(\hat{\mu}) &= \frac{1}{n} s^2 \\
&= \frac{1}{n(n-1)} \sum_{i=1}^{n} (X_i - \hat{\mu})^2
\end{aligned}
$$
This $\hat{Var}(\hat{\mu})$ is an unbiased **estimate** (hence the hat) of the variance of the sample mean $\hat{\mu}$.

The Jackknife estimate of the sample variance is given as

$$
\hat{Var}(\hat{\mu}_{jack}) = \frac{n-1}{n} \sum_{i=1}^{n} (\bar{x}_{(i)} - \hat{\mu}_{jack})^2
$$

Here $\hat{\mu}_{jack} = \frac{1}{n} \sum_{i=1}^{n} \bar{x}_{(i)}$, that is, the average of the subsample means.

The proof of this formula is beyond the scope of basic statistics here. But we can show that for the sample mean estimator case, the Jackknife formula is equivalent to the estimator $\hat{\mu}_{jack} = \frac{1}{n} \sum_{i=1}^{n} \bar{x}_{(i)}$ we just derived.

The key here is to transform $\bar{x}_{(i)} - \hat{\mu}_{jack}$ into some form of $X_i - \hat{\mu}$.

$$
\begin{aligned}
\bar{x}_{(i)} - \hat{\mu}_{jack} &= \frac{n \cdot \hat{\mu}_{jack} - X_i}{n-1} - \hat{\mu}_{jack} &(1) \\
&= \frac{n}{n-1} \hat{\mu}_{jack} - \frac{1}{n-1} X_i - \hat{\mu}_{jack} &(2) \\
&= (\frac{n}{n-1} - 1) \hat{\mu}_{jack} - \frac{1}{n-1} X_i &(3) \\
&= \frac{1}{n-1} \hat{\mu}_{jack} - \frac{1}{n-1} X_i &(4) \\
&= \frac{1}{n-1} (\hat{\mu}_{jack} - X_i) &(5)
\end{aligned}
$$

The transformation step (1) is just using the definition of $\bar{x}_{(i)}$ and the rest is straightforward algebra.

Now we can substite $\bar{x}_{(i)} - \hat{\mu}_{jack}$ in the formula.

$$
\begin{aligned}
\hat{Var}(\hat{\mu}_{jack}) &= \frac{n-1}{n} \sum_{i=1}^{n} (\bar{x}_{(i)} - \hat{\mu}_{jack})^2 \\
&= \frac{n-1}{n} \sum_{i=1}^{n} (\frac{1}{n-1} (\hat{\mu}_{jack} - X_i))^2 \\
&= \frac{n-1}{n} \cdot \frac{1}{(n-1)^2} \sum_{i=1}^{n} (X_i - \hat{\mu}_{jack})^2 \\
&= \frac{1}{n(n-1)} \sum_{i=1}^{n} (X_i - \hat{\mu}_{jack})^2
\end{aligned}
$$

We've shown the formula $\hat{Var}(\hat{\mu}_{jack}) = \frac{n-1}{n} \sum_{i=1}^{n} (\bar{x}_{(i)} - \hat{\mu}_{jack})^2$ can be applied on the sample mean estimator to estimate its variance. The same formula actually can be used to estimate variance of a wide collection of sample estimators.

Take an estimator $\hat{\theta}$, denote $\hat{\theta}_{(.)}$ as the average of all subsample estimates, $\hat{\theta}_{(i)}$.

$$
\hat{Var}(\hat{\theta})_{jack} = \frac{n-1}{n} \sum_{i=1}^{n} (\hat{\theta}_{(i)} - \hat{\theta}_{(.)})^2
$$

#### Jackknife estimate of bias

The jackknife technique can also be used to estimate the bias of an estimator calculated over the entire sample.

We know that sample mean is an unbiased estimator of the population mean, so we can't continue to use the sample mean estimator as an example here to demonstrate the Jackknife estimate of bias (it would be 0). However, we also know that the uncorrected sample variance estimator is a biased estimator of the population variance. So we will use sample variance as an example here to introduce the Jackknife bias estimate. It's an estimate because we can't directly observe the true bias of the uncorrected sample variance estimator and hence use the technique to **estimate** the bias.

The Jackknife estimate of the bias of an estimator $\hat{\theta}$ is given as

$$
\hat{bias}_{jack} = (n-1)(\hat{\theta}_{(.)} - \hat{\theta})
$$

Here $\hat{\theta}_{(.)}$ is the average of the subsample estimates. In the previous section where we use the sample mean estimator as an example, this average is denoted as $\bar{x}$, but $\hat{\theta}_{(.)}$ is more general. That is,

$$
\hat{\theta}_{(.)} = \frac{1}{n} \sum_{i=1}^{n} \hat{\theta}_{(i)}
$$

where $\hat{\theta}_{(i)}$ is the sample estimates from each subsample.

We can also apply the estimator of interest to the entire sample and get the whole-sample estimate (as opposed to those leave-out subsamples in Jackknife). We denote the overall estimate as $\hat{\theta}$.

We can think of the Jackknife estimate of the bias as the average of deviations between each subsample and the whole sample.

Let's use the uncorrected sample variance estimator as an example.

$$
\hat{\sigma^2} = \frac{1}{n} \sum_{i=1}^{n} (X_i - \hat{\mu})^2
$$

We know that the bias is $E(\hat{\sigma^2} - \sigma^2)$.

$$
\begin{aligned}
E(\hat{\sigma^2} - \sigma^2) &= E(\hat{\sigma^2}) - \sigma^2 \\
&= \frac{n-1}{n} \sigma^2 - \sigma^2 \\
&= -\sigma^2/n
\end{aligned}
$$

To derive the Jackknife estimate of this bias, we first define what $\hat{\theta}_{(.)}$ and $\hat{\theta}$ are for the sample variance estimator.

$$
\hat{\theta} = \hat{\sigma^2} = \frac{1}{n} \sum_{i=1}^{n} (X_i - \hat{\mu})^2
$$

and

$$
\hat{\theta}_{(.)} = \hat{\sigma^2_{(.)}} = \frac{1}{n} \sum_{i=1}^{n} \hat{\sigma^2_{(i)}}
$$

The Jackknife estimate of this bias is

$$
\begin{aligned}
\hat{bias}_{jack} &= (n-1) (\hat{\theta}_{(.)} - \hat{\theta}) \\
&= (n-1) (\hat{\sigma^2_{(.)}} - \hat{\sigma^2}) \\
&= (n-1)(\frac{1}{n} \sum_{i=1}^{n} \hat{\sigma^2_{(i)}} - \hat{\sigma^2})
\end{aligned}
$$

in which,

$$
\hat{\sigma^2_{(i)}} = \frac{1}{n-1} \sum_{j=1,j \neq i}^{n} (X_j - \bar{x}_i)^2
$$

Note here we use the $\frac{1}{n-1}$ term because the summation has $n-1$ terms, corresponding to the $n-1$ elements (indexed by $j$) from the sample that form the subsample.

Now in order to simplify the Jackknife bias estimate formula, we want to find a transformation to bring $\hat{\sigma^2_{(i)}}$ closer to $\hat{\sigma^2}$.

Let's look at the $X_j - \bar{x}_i$ term first.

$$
\begin{aligned}
X_j - \bar{x}_i &= X_j - \frac{n\hat{\mu} - X_i}{n-1} \\
&= X_j - \frac{(n-1)\hat{\mu} + \hat{\mu} - X_i}{n-1} \\
&= (X_j - \hat{\mu}) + \frac{1}{n-1}(X_i-\hat{\mu}) \\
\end{aligned}
$$

The left side of the equation is the difference between an individual subsample element and the subsample mean. The equation says that the difference can be split into two terms. The first is the difference between the element and the overall sample mean. The second term is the adjustment which is equal to $\frac{1}{n-1}$ of the difference between the missing element $X_i$ and the overall sample mean.

Now we express the term as the sum of two terms, both of which can be expressed as the difference between an element and the overall sample mean. We have the bridge to link $\hat{\sigma^2_{(i)}}$ to $\hat{\sigma^2}$.

$$
\begin{aligned}
\hat{\sigma^2_{(i)}} &= \frac{1}{n-1} \sum_{j=1,j \neq i}^{n} (X_j - \bar{x}_i)^2 \\
&= \frac{1}{n-1} \sum_{j=1,j \neq i}^{n} [(X_j - \hat{\mu}) + \frac{1}{n-1}(X_i-\hat{\mu})]^2 \\
&= \frac{1}{n-1} \sum_{j=1,j \neq i}^{n} [(X_j - \hat{\mu})^2 + \frac{2}{n-1}(X_j - \hat{\mu})(X_i-\hat{\mu}) + \frac{1}{(n-1)^2}(X_i-\hat{\mu})^2] \\
&= \frac{1}{n-1} [\sum_{j=1,j \neq i}^{n} (X_j - \hat{\mu})^2 + \frac{2}{n-1} \sum_{j=1,j \neq i}^{n} (X_j - \hat{\mu})(X_i-\hat{\mu}) + \frac{1}{(n-1)^2} \sum_{j=1,j \neq i}^{n} (X_i-\hat{\mu})^2] \\
\end{aligned}
$$

Let's look at the second term first.

$$
\begin{aligned}
\sum_{j=1,j \neq i}^{n} (X_j - \hat{\mu})(X_i-\hat{\mu}) &= (X_i-\hat{\mu}) \sum_{j=1,j \neq i}^{n} (X_j - \hat{\mu}) \\
&= (X_i-\hat{\mu})(\sum_{j=1,j \neq i}^{n} X_j - (n-1)\hat{\mu}) \\
&= (X_i-\hat{\mu})[(n\hat{\mu} - X_i) - (n-1)\hat{\mu}] \\
&= (X_i-\hat{\mu})(n\hat{\mu} - X_i - n\hat{\mu} + \hat{\mu}) \\
&= -(X_i-\hat{\mu})^2
\end{aligned}
$$

The third term is straightforward to transform because there's no $j$-indexed term within.

$$
\begin{aligned}
\sum_{j=1,j \neq i}^{n} (X_i-\hat{\mu})^2 &= (n-1)(X_i-\hat{\mu})^2
\end{aligned}
$$

Finally, we can look at the first term.

$$
\sum_{j=1,j \neq i}^{n} (X_j - \hat{\mu})^2 = [\sum_{j=1,j \neq i}^{n} (X_j - \hat{\mu})^2 + (X_i - \hat{\mu})^2] - (X_i - \hat{\mu})^2
$$

Now we can substitute each term's transformation into the formula.

$$
\begin{aligned}
\hat{\sigma^2_{(i)}} &= \frac{1}{n-1} [\sum_{j=1,j \neq i}^{n} (X_j - \hat{\mu})^2 + \frac{2}{n-1} \sum_{j=1,j \neq i}^{n} (X_j - \hat{\mu})(X_i-\hat{\mu}) + \frac{1}{(n-1)^2} \sum_{j=1,j \neq i}^{n} (X_i-\hat{\mu})^2] &(1) \\
&= \frac{1}{n-1} \{ [\sum_{j=1,j \neq i}^{n} (X_j - \hat{\mu})^2 + (X_i - \hat{\mu})^2] - (X_i - \hat{\mu})^2 - \frac{2}{n-1} (X_i-\hat{\mu})^2 + \frac{1}{(n-1)^2} (n-1)(X_i-\hat{\mu})^2 \} &(2) \\
&= \frac{1}{n-1} \{ [\sum_{j=1,j \neq i}^{n} (X_j - \hat{\mu})^2 + (X_i - \hat{\mu})^2] - (X_i - \hat{\mu})^2 - \frac{2}{n-1} (X_i-\hat{\mu})^2 + \frac{1}{n-1} (X_i-\hat{\mu})^2 \} &(3) \\
&= \frac{1}{n-1} \{ [\sum_{j=1,j \neq i}^{n} (X_j - \hat{\mu})^2 + (X_i - \hat{\mu})^2] - (X_i - \hat{\mu})^2 - \frac{1}{n-1} (X_i-\hat{\mu})^2 \} &(4) \\
&= \frac{1}{n-1} \{ [\sum_{j=1,j \neq i}^{n} (X_j - \hat{\mu})^2 + (X_i - \hat{\mu})^2] - (1 + \frac{1}{n-1}) (X_i - \hat{\mu})^2 \} &(5) \\
&= \frac{1}{n-1} \{ [\sum_{j=1,j \neq i}^{n} (X_j - \hat{\mu})^2 + (X_i - \hat{\mu})^2] - \frac{n}{n-1} (X_i - \hat{\mu})^2 \} &(6) \\
&= \frac{1}{n-1} [\sum_{j=1,j \neq i}^{n} (X_j - \hat{\mu})^2 + (X_i - \hat{\mu})^2] - \frac{n}{(n-1)^2} (X_i - \hat{\mu})^2 &(7) \\
&= \frac{1}{n-1} \cdot n \cdot \hat{\sigma^2} - \frac{n}{(n-1)^2} (X_i - \hat{\mu})^2 &(8) \\
&= \frac{n}{n-1} \hat{\sigma^2} - \frac{n}{(n-1)^2} (X_i - \hat{\mu})^2 &(9) \\
&= s^2 - \frac{n}{(n-1)^2} (X_i - \hat{\mu})^2 &(10)
\end{aligned}
$$

The transformation from (7) to (8) depends on the following equality.

$$
\begin{aligned}
\sum_{j=1,j \neq i}^{n} (X_j - \hat{\mu})^2 + (X_i - \hat{\mu})^2 &= \sum_{k=1}^{n} (X_k - \hat{\mu})^2 \\
&= n \cdot \hat{\sigma^2}
\end{aligned}
$$

Recall that the Jackknife estimate of the bias is

$$
\begin{aligned}
\hat{bias}_{jack} &= (n-1) (\hat{\theta}_{(.)} - \hat{\theta}) \\
&= (n-1) (\hat{\sigma^2_{(.)}} - \hat{\sigma^2}) \\
&= (n-1)(\frac{1}{n} \sum_{i=1}^{n} \hat{\sigma^2_{(i)}} - \hat{\sigma^2})
\end{aligned}
$$

We still need to sum up $n$ $\hat{\sigma^2_{(i)}}$ to get $\hat{\sigma^2_{(.)}}$.

$$
\begin{aligned}
\frac{1}{n} \sum_{i=1}^{n} \hat{\sigma^2_{(i)}} &= \frac{1}{n} \sum_{i=1}^{n} [\frac{n}{n-1} \hat{\sigma^2} - \frac{n}{(n-1)^2} (X_i - \hat{\mu})^2] \\
&= \frac{1}{n} [\frac{n^2}{n-1}\hat{\sigma^2} - \frac{n}{(n-1)^2} \sum_{i=1}^{n} (X_i - \hat{\mu})^2] \\
&= \frac{n}{n-1}\hat{\sigma^2} - \frac{1}{n} \cdot \frac{n}{(n-1)^2} \cdot n \cdot \hat{\sigma^2} \\
&= \frac{n}{n-1} \hat{\sigma^2} - \frac{n}{(n-1)^2} \hat{\sigma^2} \\
&= [\frac{n}{n-1} - \frac{n}{(n-1)^2}] \hat{\sigma^2} \\
&= \frac{n(n-1)-n}{(n-1)^2} \hat{\sigma^2} \\
&= \frac{(n-1)^2-1}{(n-1)^2} \hat{\sigma^2} \\
&= [1-\frac{1}{(n-1)^2}] \hat{\sigma^2} \\
\end{aligned}
$$

That is, the average of subsample uncorrected variance estimates is equal to the overall sample uncorrected variance estimate minus a small amount (note the term $\frac{1}{(n-1)^2}$ shrinks very fast when $n$ increases). This means $\frac{1}{n} \sum_{i=1}^{n} \hat{\sigma^2_{(i)}} < \hat{\sigma^2}$. It makes intuitive sense because $\hat{\sigma^2_{(i)}}$ is computed using subsamples which by definition is a subset of the overall sample and hence it's expected that dispersion (what variance is measuring) of a subset must be smaller than the dispersion of the overall.

Now we can finally derive the Jackknife estimate of the bias of the uncorrected sample variance estimator.

$$
\begin{aligned}
\hat{bias}_{jack} &= (n-1)(\frac{1}{n} \sum_{i=1}^{n} \hat{\sigma^2_{(i)}} - \hat{\sigma^2}) \\
&= (n-1) \{ [1-\frac{1}{(n-1)^2}] \hat{\sigma^2} - \hat{\sigma^2} \} \\
&= (n-1) \cdot \frac{-1}{(n-1)^2} \cdot \hat{\sigma^2} \\
&= \frac{-\hat{\sigma^2}}{n-1} = \frac{-s^2}{n}
\end{aligned}
$$

Thus, the Jackknife bias is constructed from a heuristic notion to emulate the bias of the uncorrected sample variance.

If we take the estimator of interest $\hat{\theta}$ to be the **corrected** sampled variance estimator $s^2$, we can show that the Jackknife estimate of the bias is 0.

$$
\hat{bias}_{jack} = (n-1)(s_{(.)}^2 - s^2) = 0
$$



